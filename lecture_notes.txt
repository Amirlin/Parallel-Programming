lecture #1
openMP --> synchronisation between threads, uses shared mem in local
MPI --> communication between threads, uses network to do a global task
Gain of parallelism --> sequential steps/nb of steps in parallel

in openMP we use reduction to protect shared data

Flynn's Taxanomy

MIMD several CPU several Memory --> via network, for each CPU there is internal cache Memory, 
shared Memory --> central mem, distibuted mem--> communication between several CPUs to share
enternal variable in the cache, use MPI for that
SISD -->all computer
MISD --> pipelineing

lecture #2

openMP
shared mem --> if many threads then bus contention leads low scalability, simpler to program
one machine (one Memory) with several CPUs

MPI -> diffucult to program

export OMP_NUM_THREADS=7

Salamç mən Fransiz Azərbaycan Universitetiç Komputer Elmləri fakultəsi 4cü kurs tələbəsiyəm.
Esasen Programlaşdirma, Kiber təhlükəsizlik saheleri ile maraqlaniram.

In other words, backpropagation aims to minimize the cost function by adjusting network's 
weights and biases. The level of adjustment
is determined by the gradients of the cost function with respect to those parameters.

Backpropagation aims to minimize the cost function by adjusting network's weights and biases. 
The level of adjustment is 
determined by the gradients of the cost function with respect to the parameters.

